{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8355c04-b7f0-4144-aa02-7c90e9203fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0CRSEN2517-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac19e0-6e45-44ac-a966-87e7d2e8ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Winning Techniques for Your Next Data Science Contest**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df473a-9c71-43ff-a2d8-1a4e5d693199",
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a001941-2dbb-4303-9117-bc4138c97b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data science competitions are a great way to learn and improve your skills in machine learning, as well as to showcase your abilities to a wider audience. However, participating in a competition can be daunting, especially if you are new to the field. \n",
    "\n",
    "In this guided project, we will go through the process of jumpstarting a data science competition, following the best practices of a machine learning project. By the end of this project, you will have a solid understanding of how to prepare for a competition, from data cleaning to model evaluation, and be ready to take on the challenge of building a successful machine learning model. \n",
    "\n",
    "Additionally, we'll dive into the world of boosting algorithms and explore three renowned algorithms: CatBoost, LightGBM, and XGBoost, which have been known to produce top-performing models. So let's get started!\n",
    "\n",
    "\n",
    "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0CRSEN/images/Screen%20Shot%202023-04-03%20at%2010.51.22%20PM.png\" width=\"60%\"></center>\n",
    "\n",
    "<p style=\"color:gray; text-align:center;\">Credit: THINKSTOCK</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e70fd-c337-4369-8a4d-de50c5f8073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Background-(optional)\">Background (optional)</a></li>\n",
    "    <li><a href=\"#Loading the dataset\">Loading the dataset</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Preprocessing the dataset (optional)\">Preprocessing the dataset (optional)</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Make Loc Groups\">Make Loc Groups</a></li>\n",
    "            <li><a href=\"#Fill Missing Values\">Fill Missing Values</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Feature Selection and Lasso Regressions (to Obtain Residuals)\">Feature Selection and Lasso Regressions (to Obtain Residuals)</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Train, Validation, Test Split\">Train, Validation, Test Split</a></li>\n",
    "            <li><a href=\"#Feature Selection For Linear Regression\">Feature Selection For Linear Regression</a></li>\n",
    "            <li><a href=\"#Selecting Alpha and Fitting Linear Regressions (to Obtain Residuals)\">Selecting Alpha and Fitting Linear Regressions (to Obtain Residuals)</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Using Boosting Algorithms to Predict Residuals\">Using Boosting Algorithms to Predict Residuals</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Feature Selection for Boosting\">Feature Selection for Boosting</a></li>\n",
    "            <li><a href=\"#Boosting Models for Predicting Fitted Residuals\">Boosting Models for Predicting Fitted Residuals</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe1516-ddd5-45b8-b901-85e80daa67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Understand the working mechanisms of three famous boosting algorithms: CatBoost, LightGBM, and XGBoost.\n",
    "- Implement these algorithms in Python and optimize their hyperparameters.\n",
    "- Evaluate algorithm performance on a real-world dataset.\n",
    "- Discover how to use ensemble learning techniques to combine model predictions and achieve even better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa5f9f-b25e-4c82-b251-a78c60e22eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7ee28-35f8-418d-8cbb-9831e86b3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536cbb77-34e8-46d4-a4c6-1fee84f0fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205c753-4d2a-44e2-8b98-6d5516189bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "55ffdd6d-ec82-43e8-a854-1e6f023a75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe1ea9-994f-4497-b84c-876899e8ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f9929a1-f1c9-4f90-8f4f-15931d27bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -qy catboost\n!mamba install -qy lightgbm\n!mamba install -qy xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a96ee-7172-457f-9d7e-47f0841b8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbd58c69-0e74-4044-a59f-b0a878a959b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport tqdm\nfrom numpy import median\nimport random\nimport catboost as cb\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.ensemble import RandomForestClassifier\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c0a4a8-63da-41f9-8544-f5aa8b51419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining Helper Functions\n",
    "\n",
    "_Use this section to define any helper functions to help the notebook's code readability:_\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "03eae802-fd07-42df-a518-eeff2e31aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, predicted):\n    return mean_squared_error(actual, predicted, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35119463-ba99-4305-a5b0-4c786661e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Background (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663db38-8015-411c-b6ae-a7438b91f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost, LightGBM, and XGBoost are three popular boosting algorithms in machine learning. Before we dive into the these three algorithms, let's start with the term ensemble learning and how boosting as one type of ensemble learning helps us reduce model errors.\n",
    "\n",
    "**Ensemble Learning** is a Machine Learning technique that combines predictions from multiple base models to produce more accurate and robust predictions. Ensemble learning can improve the generalization of the model. It can help to create a model that can generalize well to new and unseen data by combining the different perspectives of individual models. There are several ways to create an ensemble model, including **bagging, boosting, and stacking**. \n",
    "\n",
    "**Bagging** is primarily used to reduce the variance of a model by combining predictions of multiple models that are trained on different subsets of the training data through a voting mechanism. **Boosting**, on the other hand, is used to reduce the bias of a model by sequentially training models that focus on correcting the errors of the previous models. **Stacking** is used to combine multiple models by training a meta-model on the predictions of the base models to create more accurate final predictions.\n",
    "\n",
    "In this guided project, **we will focus on the Boosting algorithms**, which are particularly helpful for achieving success in data science competitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b994c-091d-4d3d-ac9e-8a916e2dcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb4d2d-9918-40ba-9886-3a3abe48e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "The dataset we will use in this project is provided by Women in Data Science (WiDS Datathon) 2023 competition on Kaggle. This compeition focuses on forecasting weather in different regions within the US over a two-week period. Each row in the data corresponds to a single location and a single start date for the two-week period. **Our task is to predict the average temperature over the next 14 days, for each location and start date**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a7a9c-5654-4bbf-ae41-94c5e553004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "We have a pre-prepared dataset consisting of features such as weather and climate information for a number of US locations, for a number of start dates for the two-week observation, as well as the forecasted temperature and precipitation from a number of weather forecast models. For more information regarding the data schema, please refer to the [competition](https://www.kaggle.com/competitions/widsdatathon2023/data?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0CRSEN2517-2023-01-01).\n",
    "\n",
    "Let's import the dataset. It may take 2-3 minutes to import as the original dataset is quite large.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0119386-156d-44e4-b13e-53c37c873f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0CRSEN/LargeData/train_data.csv\", parse_dates=['startdate'], index_col=0)\ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac3d22bd-7adb-4b61-b917-f66027a01d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2364375-4568-4b3c-b802-f1bf32b1d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'contest-tmp2m-14d__tmp2m'\ndf[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb27ce-f53c-4e1a-b25a-093f5100dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "As you can see, the original dataset contains 375,734 observations, 244 features and one target variable `contest-tmp2m-14d__tmp2m`. According to the competition, `contest-tmp2m-14d__tmp2m` is the **arithmetic mean of the max and min observed temperature over the next 14 days for each location and start date**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3088c-0fa7-459b-a455-8930d818d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing the dataset (Optional)\n",
    "\n",
    "This section explains how we preprocessed the original dataset and then split it into train, validation, and test datasets. Preprocessing is a critical step in building a successful Machine Learning project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e39ca-2d83-4fc9-b7e5-5c5ef3c9aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make Location Groups\n",
    "\n",
    "First, the location data given are expressed in terms of longitude and latitude, we can turn this critical information into a more convenient format by creating location groups. A location group has its unique combination of longitude and latitude. However, to avoid having too many different groups, we choose to round the longitudes and latitudes down to contain 14 decimal places. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3dfe37ee-6636-456e-9cb8-498f97dd14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_group(df):\n    \n    scale = 14\n\n    df['lat'] = np.round(df['lat'], scale)\n    df['lon'] = np.round(df['lon'], scale)\n    df['loc_group'] = df.groupby(['lon', 'lat']).ngroup()\n     \n    return df"
   ]
  },
  {
   "cell_type": "code",
   "id": "1950d2eb-1179-4c97-8e9e-06e2f15abd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loc_group(df)\ndf.loc_group.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8d5a8-2cb4-42ce-a42b-43bc51233da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "We created in total 514 different location groups for which we will predict the average weather in the next 14 days.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ec701f7-3b1b-4d91-be8d-0a171fbd3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_startdates = []\n\nfor n in range(df.loc_group.nunique()):\n    n_startdates.append(df[df.loc_group == n].startdate.nunique())\n\nprint(n_startdates)\nprint()\nprint(df.startdate.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a96420-9f97-479b-9540-18af3161a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see that each `loc_group` has 731 unique start dates and we have in total 731 unique start dates in the entire dataset, meaning all location groups have data for the same 731 start dates. This is a good thing because we have a balanced dataset for training models to forecast weather in different locations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db823e98-4b4b-41d5-8242-1027b4480235",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf6978-ac22-41be-b141-8869563de345",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's examine columns with missing values in the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bcfba71d-2fec-4f0b-a422-e01b5acca316",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols = []\n\nfor col in df.columns:\n    if df[col].isnull().sum() >0:\n        na_cols.append(col)\n        print(col, df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403e299-fe49-4fce-a965-f1b0059b7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "According to the competition, **cancm30, cancm40, ccsm30, ccsm40, cfsv20, gfdlflora0, gfdlflorb0, gfdl0, nasa0, and nmme0mean** are most recent forecasts from weather models. \n",
    "\n",
    "- `nmme0-prate-34w`: weeks 3-4 weighted average of most recent monthly NMME model forecasts for precipitation.\n",
    "- `nmme0-prate-56w`: weeks 5-6 weighted average of most recent monthly NMME model forecasts for precipitation.\n",
    "- `nmme-prate-34w`: weeks 3-4 weighted average of monthly NMME model forecasts for precipitation.\n",
    "- `nmme-prate-56w`: weeks 5-6 weighted average of monthly NMME model forecasts for precipitation.\n",
    "- `nmme-tmp2m-34w`: weeks 3-4 weighted average of most recent monthly NMME model forecasts for target label, `contest-tmp2m-14d__tmp2m`.\n",
    "- `nmme-tmp2m-56w`: weeks 5-6 weighted average of monthly NMME model forecasts for target label, `contest-tmp2m-14d__tmp2m`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c57c8-eaab-4d86-bc59-4fe9ca7c40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we know that we are missing forecasts from **ccsm30 and ccsm3** NMME (North American Multi-Model Ensemble) weather models for the above prefixes. \n",
    "\n",
    "We can make an educated guess that the missing forecasts can be derived from the remaining complete forecasts. Specifically, for column `nmme-tmp2m-34w__ccsm3`, its missing values can be computed through first multiplying the average forecast `nmme-tmp2m-34w__nmmemean` by 9 to get the sum of **nmme-tmp2m-34w** forecasts and then subtract the other 8 forecasts (of 8 remaining weather forecast models).\n",
    "\n",
    "We will use this methodology to fill missing values for all columns containing missing values. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b6efe0f-2536-4062-9f2c-eb14ab695afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_na_cols = ['nmme0-tmp2m-34w__ccsm30',\n 'nmme-tmp2m-56w__ccsm3',\n 'nmme-prate-34w__ccsm3',\n 'nmme0-prate-56w__ccsm30',\n 'nmme0-prate-34w__ccsm30',\n 'nmme-prate-56w__ccsm3',\n 'nmme-tmp2m-34w__ccsm3']\n\n\nforecast_mean_cols = ['nmme0-tmp2m-34w__nmme0mean', \n 'nmme-tmp2m-56w__nmmemean', \n 'nmme-prate-34w__nmmemean', \n 'nmme0-prate-56w__nmme0mean', \n 'nmme0-prate-34w__nmme0mean', \n 'nmme-prate-56w__nmmemean', \n 'nmme-tmp2m-34w__nmmemean']\n\n\nnmme0_tmp2m_34w = ['nmme0-tmp2m-34w__cancm30',\n'nmme0-tmp2m-34w__cancm40',\n'nmme0-tmp2m-34w__ccsm40',\n'nmme0-tmp2m-34w__cfsv20',\n'nmme0-tmp2m-34w__gfdlflora0',\n'nmme0-tmp2m-34w__gfdlflorb0',\n'nmme0-tmp2m-34w__gfdl0',\n'nmme0-tmp2m-34w__nasa0']\n\nnmme_tmp2m_56w = ['nmme-tmp2m-56w__cancm3',\n'nmme-tmp2m-56w__cancm4',\n'nmme-tmp2m-56w__ccsm4',\n'nmme-tmp2m-56w__cfsv2',\n'nmme-tmp2m-56w__gfdl',\n'nmme-tmp2m-56w__gfdlflora',\n'nmme-tmp2m-56w__gfdlflorb',\n'nmme-tmp2m-56w__nasa']\n\nnmme_prate_34w = ['nmme-prate-34w__cancm3',\n'nmme-prate-34w__cancm4',\n'nmme-prate-34w__ccsm4',\n'nmme-prate-34w__cfsv2',\n'nmme-prate-34w__gfdl',\n'nmme-prate-34w__gfdlflora',\n'nmme-prate-34w__gfdlflorb',\n'nmme-prate-34w__nasa']\n\nnmme0_prate_56w = [ 'nmme0-prate-56w__cancm30',\n'nmme0-prate-56w__cancm40',\n'nmme0-prate-56w__ccsm40',\n'nmme0-prate-56w__cfsv20',\n'nmme0-prate-56w__gfdlflora0',\n'nmme0-prate-56w__gfdlflorb0',\n'nmme0-prate-56w__gfdl0',\n'nmme0-prate-56w__nasa0']\n\nnmme0_prate_34w = ['nmme0-prate-34w__cancm30',\n'nmme0-prate-34w__cancm40',\n'nmme0-prate-34w__ccsm40',\n'nmme0-prate-34w__cfsv20',\n'nmme0-prate-34w__gfdlflora0',\n'nmme0-prate-34w__gfdlflorb0',\n'nmme0-prate-34w__gfdl0',\n'nmme0-prate-34w__nasa0']\n\nnmme_prate_56w = ['nmme-prate-56w__cancm3',\n'nmme-prate-56w__cancm4',\n'nmme-prate-56w__ccsm4',\n'nmme-prate-56w__cfsv2',\n'nmme-prate-56w__gfdl',\n'nmme-prate-56w__gfdlflora',\n'nmme-prate-56w__gfdlflorb',\n'nmme-prate-56w__nasa']\n\nnmme_tmp2m_34w = ['nmme-tmp2m-34w__cancm3',\n'nmme-tmp2m-34w__cancm4',\n'nmme-tmp2m-34w__ccsm4',\n'nmme-tmp2m-34w__cfsv2',\n'nmme-tmp2m-34w__gfdl',\n'nmme-tmp2m-34w__gfdlflora',\n'nmme-tmp2m-34w__gfdlflorb',\n'nmme-tmp2m-34w__nasa']\n\nforecast_cols = [nmme0_tmp2m_34w, nmme_tmp2m_56w, nmme_prate_34w, nmme0_prate_56w, nmme0_prate_34w, nmme_prate_56w, nmme_tmp2m_34w]"
   ]
  },
  {
   "cell_type": "code",
   "id": "09e595b0-be8a-46e0-90ba-a6fa4d3cf167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n\n    for i, na_col in enumerate(forecast_na_cols):\n        total = df[forecast_mean_cols[i]] * 9\n        sum_of_8_forecast = df[forecast_cols[i]].sum(axis=1)\n        df[na_col] = total - sum_of_8_forecast\n        \n    return df"
   ]
  },
  {
   "cell_type": "code",
   "id": "2eeb032c-ab9b-46cc-a424-b61bfacbe36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impute_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0886058-122f-4278-8940-e51c4f639eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "We will just drop the `ccsm30` column as it's redundant.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "63141ebb-dedb-4941-a42b-7f744685e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_col(df):\n    \n    df.drop(['ccsm30'], axis=1, inplace=True)\n    \n    return df"
   ]
  },
  {
   "cell_type": "code",
   "id": "300664bf-22a1-4c46-8f97-29ba1967beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_redundant_col(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e7ac0-4bcf-439c-ae51-908fa7dcd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we no longer have missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc3432df-c232-4daf-8e5f-c5e3fd93b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16922c54-0338-4fbe-bff2-9e6ff265a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection and Lasso Regressions (to Obtain Residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84490dee-fc39-4fbf-96c7-56d3cd6ebd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting algorithms are normally used to predict the residuals of the base models' predictions. To obtain these residuals, we can first fit linear regressions to the data with some regularization. To account for the fact that different locations might have different relationships between its predictors and its response variable, we fit 514 different linear regressions and collect their residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9bc73-a00d-4917-bbbb-d7af604dd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train, Validation, Test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1c7e1-1145-45dc-9455-daaa9a1be95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "We sort the data used for fitting linear regressions by location group and start date. We split the data so that for each location group, data between 2014-09-1 and 2015-11-15 are used for training, data between 2015-11-15 and 2016-04-08 are used for validation, and data between 2016-04-08 and 2016-08-31 are used for testing. By doing so, we maintain a **6:2:2** ratio of train, validation, and test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea5b7c47-52a3-4e66-a919-88a7c8cf4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['loc_group', 'startdate'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "id": "0eefaadc-88d9-48e6-af4d-b1fbb45d874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "id": "0e46d2ef-c4af-4576-a234-066ab4052af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df[\"startdate\"] <= pd.datetime(2015, 11, 15)]\nval = df[(df[\"startdate\"] > pd.datetime(2015, 11, 15)) & (df[\"startdate\"] <= pd.datetime(2016, 4, 8))]\ntest = df[df[\"startdate\"] > pd.datetime(2016, 4, 8)]\n\nprint(f'Train_shape: {train.shape}  |  Val_shape: {val.shape}   |  Test_shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7682f-d04f-4f1d-99c2-d76eff98fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Selection For Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec6ed8-d250-470a-974d-b64d41491897",
   "metadata": {},
   "outputs": [],
   "source": [
    "Since we are fitting linear regression models, predictors that have high linear correlation with the response variable will give us better model fitting results. Thus, we select some features based on this criterion, hoping that the selected features could explain any underlying linear relationships.\n",
    "\n",
    "The first set of potential features we will look at are the ones whose names contain \"14d\", which indicates that they are measured over the same time frame as the target variable and thus they have higher chance of being linearly correlated with the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c12b929-d7e0-45f4-9146-c3f8fcaef663",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cols = []\n\nfor col in df.columns:\n    if '14d' in col and col != target:\n        corr = df[[col, target]].corr().iloc[0,1]\n        if np.abs(corr) > 0.8: \n            print(col, corr)\n            LR_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2a614-4bda-4ba6-80b6-b5eaca5c672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "By setting 0.8 as the threshold, we identified three features with high linear correlations with the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714ad05-6454-4ad6-91ba-a8f60c836d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another potential set of features are the NMME model forecasts as they are purely physics-based models which forecast weather-related information such as precipitation. Hence, they have the potential of providing us with some meaningful insights. We have the following prefixes, each of them is associated with the NMME model forecasts over a specific time frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9520d2a5-ab4f-4cb8-99c8-1e5756603518",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixs = []\n\nfor col in df.columns:\n    if '34w' in col or '56w' in col:\n        prefix = col.split(\"__\")[0]\n        prefixs.append(prefix)\n\nprefixs = list(set(prefixs))\nprefixs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e501b09-5459-4f1e-9af2-f05cdec4a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "Since there are many columns sharing the same prefix, let's see if information contained in columns sharing a prefix can be \"summarized\" by linear PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f531df1-1860-46cc-b431-b78257021a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\ny = np.array(df[target]).reshape(1, -1)\n\nfor prefix in prefixs:\n    \n    cols = [col for col in df.columns if prefix in col and 'mean' not in col]\n    pca1 = pca.fit_transform(df[cols]).reshape(1, -1)\n    print(prefix, f\"| pca1 explained variance: {pca.explained_variance_ratio_[0]}\")\n    print(f\"pca1 vs target corr: {np.corrcoef(pca1, y)[1,0]}\")\n    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e0eac-9651-454d-bc0d-b50511f0869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "From the above result, we can conclude that for \"nmme0-tmp2m-34\", \"nmme-tmp2m-56w\", and \"nmme-tmp2m-34w\" forecasts, the contained information can be summarized by the first principal component and the first principal component is highly correlated with the target (|correlation| > 0.8). \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5946b64-2b47-4b63-94c4-d45f8b7b570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_prefix = ['nmme0-tmp2m-34w', 'nmme-tmp2m-34w', 'nmme-tmp2m-56w']\n\ncorr_prefix_cols = [nmme0_tmp2m_34w, nmme_tmp2m_34w, nmme_tmp2m_56w]\n    \nfor prefix, cols in zip(corr_prefix, corr_prefix_cols):\n    pc_name = prefix + '-pc'\n    pca = PCA(n_components=1)\n    pca.fit(df[cols])\n    for ds in [train, val, test]:\n        ds[pc_name] = pca.fit_transform(ds[cols])"
   ]
  },
  {
   "cell_type": "code",
   "id": "e571fb4a-b51a-48bd-8836-386b17f1d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cols = LR_cols + [prefix+'-pc' for prefix in corr_prefix]\nLR_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900677b3-5e9f-4e09-96ba-c7311839b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we have 6 continuous variables for fitting linear regressions, we can examine the remaining variables and select them as features if they have high linear correlations with our target.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac621733-afe7-4abd-969c-0e9368fcc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n    \n    if df[col].dtype == 'float64' and col not in LR_cols and 'nmme0-tmp2m-34w' not in col and 'nmme-tmp2m-34w' not in col and 'nmme-tmp2m-56w' not in col:      \n        corr = df[[col, target]].corr().iloc[0,1]\n        if np.abs(corr) > 0.8:\n            LR_cols.append(col)\n            print(col, corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d4bee-ec5b-43c1-afd2-afcdc01ec6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "We now have 10 continuous predictors for fitting linear regressions. They are:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "40e93d09-8d89-4dca-8d0e-759bebe07a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cols = [col for col in LR_cols if col != target]\nLR_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f6cc7-0443-4be8-bef3-11563d3925d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selecting Alpha and Fitting Linear Regressions (to Obtain Residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092472b4-f005-4c35-b120-ff1015ccf808",
   "metadata": {},
   "outputs": [],
   "source": [
    "We use the following defined function `LR_models` to help us fit 514 linear regressions to the weather data of 514 different locations. Notice that the function takes three parameters:\n",
    "\n",
    "- `alpha`: float, regularization parameter shared by the 514 linear regressions\n",
    "\n",
    "- `selected_feats`: list of strings, selected features for fitting linear regressions to the data\n",
    "\n",
    "- `shift_feats`: boolean, True if we want to shift the `..-34w-..` and `..-56w-..` features backward by 14 and 28 days respectively, by location group.\n",
    "\n",
    "The function will return the training rmse and validation rmse.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f73e8b2-f41b-40cb-afcb-971c3c5c4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_models(alpha, selected_feats, shift_feats):\n    \n    # new train and vaidation dataset containing only the selected features\n    X_train, y_train = train[selected_feats], train[target]\n    X_val, y_val = val[selected_feats], val[target]\n\n    train_se = 0\n    val_se = 0\n\n    # loop through 514 different locations, \n    # for each location we create a machine learning pipeline and fit a linear regression model\n    for i in range(df.loc_group.nunique()):\n\n        # train and validation dataset for the ith location\n        X1_train = X_train[train[\"loc_group\"]==i]\n        y1_train = y_train[train[\"loc_group\"]==i]\n        X1_val = X_val[val[\"loc_group\"]==i]\n        y1_val = y_val[val[\"loc_group\"]==i]\n\n        # collect the indices of the observations, which we will use to recover the train and val split of the data\n        t_index = X_train[train[\"loc_group\"]==i].index\n        v_index = X_val[val[\"loc_group\"]==i].index\n\n        # if shift_feats, we combine the train and val set to shift the \"34w\" and \"56w\" features \n        if shift_feats == True and np.any(['34w' in col or '56w' in col for col in selected_feats]):\n            X1_conc = pd.concat([X1_train, X1_val])           \n            for col in selected_feats:\n                if '34w' in col:\n                    X1_conc[col] = X1_conc[col].shift(-14).ffill()\n                if '56w' in col:\n                    X1_conc[col] = X1_conc[col].shift(-28).ffill()\n\n            # we recover the train and val set by their respective list of indices\n            X1_train = X1_conc.loc[t_index, :]\n            X1_val = X1_conc.loc[v_index, :]\n        \n        # scale the data\n        ss = StandardScaler()\n        X1_train_ss = ss.fit_transform(X1_train)\n        X1_val_ss = ss.transform(X1_val)\n\n        # fit the linear regression model with alpha as the regularization parameter\n        LR = Lasso(alpha=alpha, max_iter=10000)\n        LR.fit(X1_train_ss, y1_train)\n        y1_train_pred = LR.predict(X1_train_ss)\n        y1_val_pred = LR.predict(X1_val_ss)\n        \n        # accumulate the total squared errors by the sum of squared errors from the ith location\n        train_se += mean_squared_error(y1_train, y1_train_pred) * len(t_index)\n        val_se += mean_squared_error(y1_val, y1_val_pred) * len(v_index)\n        \n    # return the root mean squared error of train and validation set\n    train_rmse = np.round(np.sqrt(train_se / X_train.shape[0]), 3)\n    val_rmse = np.round(np.sqrt(val_se / X_val.shape[0]), 3)\n    \n    return train_rmse, val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876890f-513d-4b2c-bf2a-486817a56190",
   "metadata": {},
   "outputs": [],
   "source": [
    "One important parameter in fitting Linear Regressions is the `alpha` parameter. Here we are going to try a list of different alphas and then look at the root mean squared error of the training and validation dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8d11d5e-1f30-44c3-91cd-cafc604acc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmses = []\nval_rmses = []\nalphas = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 0.8, 1, 1.5, 2, 2.5, 3, 3.5, 4]\n\nfor alpha in alphas:\n    \n    print(f\"alpha = {alpha}...\")\n    train_rmse, val_rmse = LR_models(alpha, LR_cols, True)\n    train_rmses.append(train_rmse)\n    val_rmses.append(val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c707162-a5cf-4de2-aabd-2ce3811c1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, train_rmses, 'o-', label=\"train rmse\")\nplt.plot(alphas, val_rmses,'o-', label=\"val rmse\")\nplt.plot(alphas, np.array(val_rmses)-np.array(train_rmses), 'o-', label=\"val-train rmse\")\n\nfor i in np.arange(8, len(alphas)):\n    plt.annotate(f\"{np.round(val_rmses[i]-train_rmses[i], 2)}\", (alphas[i], val_rmses[i]-train_rmses[i]))\n\nplt.xlabel(\"lasso regression alpha\")\nplt.ylabel(\"rmse\")\nplt.legend(loc='best')\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13882ad6-eaf6-4d50-bff7-a3fac5f7e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "We want the error value to be as small as possible but we also don't want the models to overfit to the training data. Thus, to take both the error metric and the overfitting problem into account, we decide to use **alpha = 2** to create the linear regression residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5d713-6547-4c77-aa1c-fd1181a61b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "To obtain the residuals from the linear regressions, we are going to modify the `LR_models` function. Note that we don't need the `alpha` argument in the function anymore as it's now a fixed value.\n",
    "\n",
    "To have more training data to learn the 514 linear regressions, we are going to combine `train` and `val` into one new training dataset and we call it `train2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "edf5f6a9-9a69-47e1-8f01-f6a73f239978",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.concat([train, val], axis=0)\nprint(f'Train2_shape: {train2.shape}  |  Test_shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a5173-4d1d-4ec8-9a3f-bcdb8d92cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here is the modified function for obtaining linear regression residuals. It will return the fitted residuals for the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "63563e92-48c8-4be8-8e1d-fa927176329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_models_residuals(selected_feats, shift_feats):\n    \n    # new train and test dataset containing only the selected features\n    X_train, y_train = train2[selected_feats], train2[target]\n    X_test, y_test = test[selected_feats], test[target]\n\n    pred_residuals = np.zeros(df.shape[0])\n\n    # loop through 514 different locations, \n    # for each location we create a machine learning pipeline and fit a linear regression model\n    for i in range(df.loc_group.nunique()):\n\n        # train and test dataset for the ith location\n        X1_train = X_train[train2[\"loc_group\"]==i]\n        y1_train = y_train[train2[\"loc_group\"]==i]\n        X1_test = X_test[test[\"loc_group\"]==i]\n        y1_test = y_test[test[\"loc_group\"]==i]\n\n        # collect the indices of the observations, which we will use to recover the train and test split of the data\n        t_index = X_train[train2[\"loc_group\"]==i].index\n        te_index = X_test[test[\"loc_group\"]==i].index\n\n        # if shift_feats, we combine the train and test set to shift the \"34w\" and \"56w\" features \n        if shift_feats == True and np.any(['34w' in col or '56w' in col for col in selected_feats]):\n            X1_conc = pd.concat([X1_train, X1_test])           \n            for col in selected_feats:\n                if '34w' in col:\n                    X1_conc[col] = X1_conc[col].shift(-14).ffill()\n                if '56w' in col:\n                    X1_conc[col] = X1_conc[col].shift(-28).ffill()\n\n            # we recover the train and test set by their respective list of indices\n            X1_train = X1_conc.loc[t_index, :]\n            X1_test = X1_conc.loc[te_index, :]\n        \n        # scale the data\n        ss = StandardScaler()\n        X1_train_ss = ss.fit_transform(X1_train)\n        X1_test_ss = ss.transform(X1_test)\n\n        # fit the linear regression model with alpha = 2.0 as the regularization parameter\n        LR = Lasso(alpha=2.0, max_iter=10000)\n        LR.fit(X1_train_ss, y1_train)\n        y1_train_pred = LR.predict(X1_train_ss)\n        y1_test_pred = LR.predict(X1_test_ss)\n        \n        # return predicted residuals\n        pred_residuals[t_index] = y1_train - y1_train_pred\n        pred_residuals[te_index] = y1_test - y1_test_pred\n    \n    return pred_residuals"
   ]
  },
  {
   "cell_type": "code",
   "id": "a5e94c83-fc06-46e4-b3e5-f89032fa7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_residuals = LR_models_residuals(LR_cols, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132e2bc-c98b-4547-b107-d25453273783",
   "metadata": {},
   "outputs": [],
   "source": [
    "We confirm that we have obtained linear regression fitted residuals for all observations in the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e93ac4ec-e6c4-45c8-9f4e-2d2cee4e2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_residuals) == df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce589e8-0606-40f0-909e-17413bd3b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Boosting Algorithms to Predict Residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc9f24-346b-4215-912e-e48152555cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "First, we concatenate the fitted residuals to the dataframe and make sure that there are no null values in the `residuals` column as it will be our target column when fitting the Boosting models.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f0af69e-9116-4170-ba45-ae4092c4e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['residuals'] = pred_residuals\ndf.residuals.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cc8a0-577d-4779-9e35-0eedce3f8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "We add the `residuals` column to the training and test dataset as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0282907-f4c6-4bb4-b961-77c3b31f7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2['residuals'] = df.loc[train2.index, 'residuals']\ntest['residuals'] = df.loc[test.index, 'residuals']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5da2bd-95ab-4fb0-9d59-fd69bb2e88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Selection for Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f4571-e3b5-4d84-b9ad-698d5a3d373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we need to select features for the Boosting models, which we will call `B_cols`. First, we will use the forecasts from the purely physics-based weather models and we will select the forecast means this time.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2fea69be-ffd9-4472-86bc-b40a87b22c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_means = [col for col in df.columns if 'mean' in col]\nforecast_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a82b19-42a9-4730-82c5-05d8657085cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "As we did before with linear regressions, we are going to shift the `..-34w-..` and `..-56w-..` features backward by 14 and 28 days respectively, by location group, and then we add those shifted features to `B_cols`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "06f37765-8cae-4b86-a13a-174cdc2a139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_cols = []\n\nfor col in forecast_means:\n    if '34w' in col:\n        df[col+'_shifted'] = df.groupby(['loc_group'])[col].shift(-14).ffill()\n        B_cols.append(col+'_shifted')\n    elif '56w' in col:\n        df[col+'_shifted'] = df.groupby(['loc_group'])[col].shift(-28).ffill()\n        B_cols.append(col+'_shifted')   \n    else:   \n        B_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c2ca0-bdfc-4266-b1e9-fdba104d7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we append these shifted features to our training and test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e29e9ef6-7430-4bd4-860b-26dc4b647397",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [train2, test]:\n    for col in B_cols:\n        ds[col] = df.loc[ds.index, col]\n\nprint(train2.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c90f23-f1fe-40b2-ac58-e38fad4212e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Another set of continuous variables we could add to `B_cols` are the ones whose names contain \"14d\", because they are measured over the same time interval as our original target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "490b59d8-ffce-4c99-ad93-8eb89001eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n    if '14d' in col:\n        B_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c691ab-2f81-402d-9f73-b56686c65eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apart from the continuous variables, don't forget about the categorical variables that we haven't touched on so far. The original dataset comes with one categorical variable called `climateregions__climateregion`. We can encode this variable using Label Encoder from scikit-learn. **Note that we can analyze the feature importance later after we fit the models**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "94d4942e-52b3-4bc0-9c1c-1edf534e6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = LabelEncoder()\nLE.fit(train2['climateregions__climateregion'])\ntrain2['climateregions__climateregion'] = LE.transform(train2['climateregions__climateregion'])\ntest['climateregions__climateregion'] = LE.transform(test['climateregions__climateregion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd45a4c-0707-42ec-83dc-dd7acf18b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can create a `month` variable to see if some months have larger fitted residuals than the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "193ab17c-ed57-413a-b365-184032b443b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['startdate'].dt.month\ndf.groupby(['month'])['residuals'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada27ed-f6aa-4b76-ad74-f78ae92a0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "It appears that our linear regression fitted residuals differ quite perceivably for different months, so we could consider `month` as a feature for the Boosting models. Let's create `month` variable for the training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "51de2311-3d53-45b6-b922-e718d9604508",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [train2, test]:\n    ds['month'] = ds['startdate'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872558bf-671f-4946-bed6-526e81a9c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "`loc_group` is also an important categorical variable as it specifies the location of the weather data. According to the competition, `elevation__elevation` is also an important feature, so we will add them to `B_cols`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5864d195-40b7-41e2-9a73-5802223dc2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_cols += ['climateregions__climateregion', 'month', 'loc_group', 'elevation__elevation']\n\nprint(f\"We have {len(B_cols)} features for fitting the Boosting models.\")\nB_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab8a4a-2589-4b5b-91cc-112e0af726d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Boosting Models for Predicting Fitted Residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71299e80-9af9-41c1-a036-2c083c51009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "As we are no longer looking to explain any linear relationships between the predictors and the target variable (as they have been handled by the linear regressions we fitted), we will move on to using some Boosting models includeing CatBoost, LightGBM, and XGBoost to capture complex non-linear relationships between the input features and the fitted residuals. \n",
    "\n",
    "We will use a combination of three Boosting models to predict our fitted residuals, so basically we are trying to see if there is any non-linear pattern in the fitted residuals that we could explain using Boosting models. If the patterm we found is generalizable to unseen data, we can further reduce the gap between what Machine Learning models can predict and the ground truths. \n",
    "\n",
    "- **CatBoost**: CatBoost works by iteratively building a set of decision trees. At each iteration, it calculates the gradient of the loss function with respect to the current set of predictions, and fits a decision tree to the negative gradient. The predictions from the new decision tree are then added to the current predictions, and the process is repeated for a specified number of iterations. CatBoost also incorporates a unique mechanism for handling categorical variables, which it achieves by using an ordered boosting technique that sorts the categories in a way that optimizes the loss function.\n",
    "\n",
    "- **LightGBM**: LightGBM, on the other hand, uses a different approach for building decision trees. Instead of using a depth-first approach like many other Boosting models, LightGBM uses a leaf-wise approach that grows the tree by expanding the leaf with the largest loss reduction. This approach can be more efficient than depth-first approaches because it reduces the number of data points that need to be processed for each split. LightGBM also uses the Gradient-based One-Side Sampling (GOSS) technique to sample the most important instances for each iteration, which further improves efficiency.\n",
    "\n",
    "- **XGBoost**: XGBoost also builds decision trees, but it uses a slightly different algorithm. Like CatBoost and LightGBM, XGBoost uses a gradient-based approach to build decision trees. However, it also includes several advanced features such as regularization techniques, early stopping, and a second-order approximation of the gradient to improve prediction accuracy. XGBoost also allows for parallel processing, which can be useful for large-scale datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05b94e-516b-47d1-8c96-0d388c4700ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0CRSEN/images/1_V-vikG-ye2Y03R943d0CPw.jpeg\" width=\"50%\"></center>\n",
    "\n",
    "<p style=\"color:gray; text-align:center;\">Credit: enjoyalgorithms.com</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353467a6-4be1-4f1d-822f-50d5e72920cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "We need to specify catrgorical columns for the three Boosting models.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ebb1b41-0140-4c8b-8ebd-d8986d85e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols =  ['climateregions__climateregion', 'month', 'loc_group']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437b38c-f1aa-4702-a7f3-09d780627169",
   "metadata": {},
   "outputs": [],
   "source": [
    "We use the `boosting` function to help us fit three Boosting models at one. The function takes in three parameter:\n",
    "\n",
    "- `ratios`: list of floats, specifying the weighting of each model in the final predictions.\n",
    "\n",
    "- `selected_cols`: list of strings, selected features for fitting the three Boosting models.\n",
    "\n",
    "- `cat_cols`: list of strings, names of categorical columns in `selected_cols`.\n",
    "\n",
    "The function will return the predicted residuals for the training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8bbe32d-e06b-48b8-95f1-1f2e781cc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting(ratios=[0.6, 0.3, 0.1], selected_cols=B_cols, cat_cols=cat_cols):\n    \n    X_train, y_train = train2[selected_cols], train2['residuals']\n    X_test, y_test = test[selected_cols], test['residuals']\n\n    for ds in [X_train, X_test]:\n        ds['loc_group'] = ds['loc_group'].astype(\"category\")\n        ds['climateregions__climateregion'] = ds['climateregions__climateregion'].astype(\"category\")\n        ds['month'] = ds['month'].astype(\"category\")\n\n\n    cat_inds = [selected_cols.index(col) for col in cat_cols]\n\n    train_pred_cb, train_pred_lgb, train_pred_xgb = [np.zeros(len(y_train))] * 3\n    test_pred_cb, test_pred_lgb, test_pred_xgb = [np.zeros(len(y_test))] * 3\n\n    if ratios[0] > 0:\n        cbr = cb.CatBoostRegressor(loss_function=\"RMSE\", random_state=42, verbose=1)  \n        cbr.fit(X_train, y_train, cat_features=cat_inds)\n        train_pred_cb = cbr.predict(X_train)\n        test_pred_cb = cbr.predict(X_test)\n    if ratios[1] > 0:\n        lgbr = lgb.LGBMRegressor(objective='regression', metric='rmse', random_state=42, verbose=1)       \n        lgbr.fit(X_train, y_train, categorical_feature=cat_inds)\n        train_pred_lgb = lgbr.predict(X_train)\n        test_pred_lgb = lgbr.predict(X_test)\n    if ratios[2] > 0:\n        xgbr = xgb.XGBRegressor(objective=\"reg:squarederror\", eval_metric='rmse', enable_categorical=True, random_state=42, verbose=0)\n        xgbr.fit(X_train, y_train)\n        train_pred_xgb = xgbr.predict(X_train)\n        test_pred_xgb = xgbr.predict(X_test)\n\n    train_pred = ratios[0] * train_pred_cb + ratios[1] * train_pred_lgb + ratios[2] * train_pred_xgb\n    test_pred = ratios[0] * test_pred_cb + ratios[1] * test_pred_lgb + ratios[2] * test_pred_xgb\n\n    print(f\"train rmse = {rmse(y_train, train_pred)}, test rmse = {rmse(y_test, test_pred)}\")\n    \n    return train_pred, test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd10ae0-d20f-44f2-a0eb-37fcc31a8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "The predicted residuals are the part of the fitted residuals that could be captured by the complex non-linear Boosting models. The difference between the predicted residuals and our linear regression fitted residuals will be the final error of our predictions.\n",
    "\n",
    "In the next cell, we try three combination of ratios to see which one of CatBoost, LightGBM, and XGBoost results in the smallest training and test root mean squared error. Once you see the three Boosting models' independent performances, you can try to tweak the `ratios` parameter to achieve better result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab38f2-8ca4-490f-878e-be615a8f1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Please note that training in Skills Network Labs environment takes time, so I would advise you to download this notebook and perform training locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "70a2b682-5bc4-4bd3-a75a-fbaccd79216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pure CatBoost:\")\ntrain_pred1, test_pred1 = boosting(ratios=[1, 0, 0])\nprint(\"Pure LightGBM:\")\ntrain_pred2, test_pred2 = boosting(ratios=[0, 1, 0])\nprint(\"Pure XGBoost:\")\ntrain_pred3, test_pred3 = boosting(ratios=[0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8431e-19f7-42fd-873d-5427818eac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Congratulations! You have completed this guided project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34513ac-6f6f-4373-b0a2-52bbe358003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "This guided project demonstrates one way of jump-starting your Data Science competition. Data Science or Machine Learning projects should always follow the following cycle:\n",
    "\n",
    "- **Problem formulation**: defining the problem and understanding the business context.\n",
    "\n",
    "- **Data collection**: gathering and acquiring data from various sources.\n",
    "\n",
    "- **Data preprocessing**: cleaning, transforming, and preparing data for analysis.\n",
    "\n",
    "- **Exploratory data analysis**: understanding the structure, contents, and patterns in the data through visualizations and summary statistics.\n",
    "\n",
    "- **Feature engineering**: creating new features from the existing data.\n",
    "\n",
    "- **Model selection**: choosing an appropriate model or algorithm for the problem.\n",
    "\n",
    "- **Model training**: fitting the model to the data and tuning its hyperparameters.\n",
    "\n",
    "- **Model evaluation**: assessing the performance of the model using appropriate metrics and techniques.\n",
    "\n",
    "- **Deployment**: integrating the model into a production environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bb88f-7daa-4059-bab6-8723753b71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "However, please keep in mind that at each step of the cycle, there exists a million of different solutions and tools for solving the problem. It's a rapidly evolving field, so there is on one fixed set of winning techniques, but as long as you keep learning and practicing you will eventually develop your own set of rules for your Data Science game! Happy Learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a71aaa-d1f0-4295-9a30-6716d8b7eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e946f51-fd09-47f7-b2a2-7ba9f5c1f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0CRSEN2517-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af7944-d413-41dc-a34d-e0b6577b8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12a2e0f-0353-4271-8cca-e31236cafac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Contributor with Link](contributor_linl), Contributor No Link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7a6e6-b530-4a2d-b5d9-396f6a403ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163e560-f7cc-4008-8eec-9d603e7f4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2020-07-17|0.1|Sam|Create Lab Template|\n",
    "|2023-04-03|0.1|Roxanne L.|Create Lab First Draft|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456b2ed-1d49-4eab-bc73-743222713a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Copyright  2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
