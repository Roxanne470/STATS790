{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d19c0ab",
   "metadata": {},
   "source": [
    "# Multi-class classification with Linear Support Vector Machine from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087aea88",
   "metadata": {},
   "source": [
    "In this project, we will perform three tasks:\n",
    "\n",
    "1. Code a binary Support Vector Machine classifier with linear kernel from scratch. \n",
    "\n",
    "2. Build a multi-class linear SVM classifier on top of the linear SVM to enable the classifier to handle multi-class classification tasks.\n",
    "\n",
    "3. Compare the multi-class linear SVM classifier with the off-the-shelf linear SVM classifier from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95116655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from autograd import grad\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bfaf7",
   "metadata": {},
   "source": [
    "## 1. Implementing linear SVM from scratch for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1033550",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744e038",
   "metadata": {},
   "source": [
    "The SVM for binary classification defines a hyper-plane in a high dimensional space to separate the data points belonging to two classes. \n",
    "\n",
    "Denote by $X$ the matrix with $n$ observations and $p$ features, w is a vector orthogonal to the hyperplane and b is the bias, defining the hyperplane relative to the origin in the high-dimensional space, the separation equation is:\n",
    "\n",
    "$$ w^T x + b = 0, w \\in \\mathbb{R}^{p}, x \\in \\mathbb{R}^{p}, b \\in \\mathbb{R} $$\n",
    "\n",
    "\n",
    "\n",
    "The **support vectors** are the data points with the minimum distance to the hyperplane. The distance between the support vector and the hyperplane is called the **margin**. To optimize a SVM, the objective is to maximize the margin:\n",
    "\n",
    "$$ \\underset{w, b}{\\max} M $$\n",
    "\n",
    "**subject to:**\n",
    "- $y_i(w^T x_i + b) \\ge M, \\forall i = 1..n$\n",
    "- $\\Vert w \\Vert = 1$\n",
    "\n",
    "For $y_i \\in \\{-1, 1\\}$, $y_i(w^T x_i + b)$ defines the absolute distance between the $i$th data point to the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e30c5",
   "metadata": {},
   "source": [
    "Equivalently, by changing the condition on the norm of $w$ such that $\\Vert w \\Vert = \\frac 1M$, we can set the objective to be \n",
    "\n",
    "$$ \\min_{w, b} \\frac 12 \\Vert w \\Vert^2 $$\n",
    "\n",
    "**subject to:** $y_i(w^T x_i + b) \\ge 1, \\forall i = 1..n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6d8b1",
   "metadata": {},
   "source": [
    "However, the requirement of $y_i(w^T x_i + b) \\ge 1, \\forall i = 1..n$ may not always be possible to satisfy since it does not allow any misclassifications. In the case where it can not be satisfied, we will fail to find a hyperplane to separate the data points.\n",
    "\n",
    "Therefore, to allow for misclassification, we relax the hard constraint and update the objective function as follows:\n",
    "\n",
    "$$ \\min_{w, b} \\frac 12 \\Vert w \\Vert^2 + C \\sum_{i=1}^n \\xi_i$$\n",
    "\n",
    "**subject to:**\n",
    "\n",
    "- $\\xi_i \\ge 0$\n",
    "- $y_i(w^T x_i + b) \\ge 1 - \\xi_i, \\forall i = 1..n$\n",
    "\n",
    "where $\\xi_i$ represents the distance to the correct margin when the data point is misclassfied and $C$ is the regularization parameter that controls the level of penality applied when data points are misclassfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2d79c",
   "metadata": {},
   "source": [
    "To optimize the objective function, we write the loss function as follows:\n",
    "\n",
    "$$ L(w, b) = \\frac 12 \\Vert w \\Vert^2 + C \\sum_{i=1}^n \\max(0, 1 - y_i(w^T x_i + b)) $$\n",
    "\n",
    "We will use the gradient descent algorithm to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c4da5",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "To implement a binary linear SVM, we write a Python class which is initiated with 4 parameters:\n",
    "\n",
    "- C: the regularization parameter\n",
    "\n",
    "- alpha: learning rate for the gradient descent algorithm\n",
    "\n",
    "- max_its: number of iterations for updating the weights\n",
    "\n",
    "- random_state: random seed for initializing w_0\n",
    "\n",
    "If the `fit` function is called, the loss function of the model will be optimized or minimized using gradient descent. We record the optimal weights and bias that lead to the minimum loss (cost), for prediction use.\n",
    "\n",
    "If the `predict` function is called, we will compute the model's predicted values for the input observations using the best fitted weights and bias. We use $w^T x + b = 0$ as the separating criterion: if $w^T x + b \\ge 0$, predicted label is 1; else -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65845b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLinearSVM:\n",
    "    \n",
    "    def __init__(self, C, alpha, max_its, random_state):\n",
    "        \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_its = max_its\n",
    "        self.random_state = random_state\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        N = X.shape[1]\n",
    "        X = X.T\n",
    "        y = y.T\n",
    "        \n",
    "        def model(w, x):\n",
    "            \n",
    "            return w[:-1].T @ x + w[-1]\n",
    "        \n",
    "        def loss(w):\n",
    "\n",
    "            # loss function\n",
    "            L = 0.5 * w[:-1].T * w[:-1] + self.C * np.sum(np.maximum(0, 1 - y * model(w, X)))\n",
    "            \n",
    "            return L[0]\n",
    "        \n",
    "        def gradient_descent(g, alpha, max_its, w):\n",
    "            \n",
    "            gradient = grad(g)\n",
    "            weight_history = [w]     # container for weight history\n",
    "            cost_history = [g(w)]    # container for corresponding cost history\n",
    "            \n",
    "            for i in range(max_its):\n",
    "                # evaluate the gradient, store current weights and cost function value\n",
    "                grad_eval = gradient(w)\n",
    "\n",
    "                # take gradient descent step\n",
    "                w = w - alpha * grad_eval\n",
    "\n",
    "                # record weight and cost\n",
    "                weight_history.append(w)\n",
    "                cost_history.append(g(w))\n",
    "                \n",
    "            return weight_history,cost_history\n",
    "\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(123)\n",
    "        \n",
    "        # randomly intialize w\n",
    "        w0 = np.random.normal(size=N+1)\n",
    "        \n",
    "        # perform gradient descent\n",
    "        w_hist, c_hist = gradient_descent(g=loss, alpha=self.alpha, max_its=self.max_its, w=w0)\n",
    "        \n",
    "        # w_best should have the smallest cost value\n",
    "        ind = np.argmin(c_hist)\n",
    "        w_best = w_hist[ind]\n",
    "        \n",
    "        self.weight = w_best[:-1]\n",
    "        self.bias = w_best[-1]\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = X.T\n",
    "        preds = self.weight.T @ X + self.bias\n",
    "        labels = [1 if pred >= 0 else -1 for pred in preds] \n",
    "        pred_labels = np.array(labels)\n",
    "        \n",
    "        return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c699a04",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5dc63",
   "metadata": {},
   "source": [
    "We will use the breast cancer dataset from scikit-learn to train and test our binary classifier. The breast cancer dataset has two classes: benign (1) and malignant (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7e36e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b22102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19635cb7",
   "metadata": {},
   "source": [
    "Change 0 to -1 so that $y_i \\in \\{-1, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7e8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i] == 0:\n",
    "        y[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ea82c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    357\n",
       "-1    212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac565ea9",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d513e",
   "metadata": {},
   "source": [
    "We keep 70% of the data as training set and 30% of the data as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b511e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30) (171, 30) (398,) (171,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031c6de",
   "metadata": {},
   "source": [
    "We scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43e28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32072aae",
   "metadata": {},
   "source": [
    "### Linear SVM model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de096e",
   "metadata": {},
   "source": [
    "We create a binary linear SVM classifier object from the `BinaryLinearSVM` class and use it to fit the standardized training data `X_train_ss` and `y_train`. We set the regularization parameter to 1.0, gradient descent learning rate to 0.01, and number of iterations to 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f21d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarySVM = BinaryLinearSVM(C=1, alpha=0.01, max_its=2000, random_state=123)\n",
    "binarySVM.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95983dbb",
   "metadata": {},
   "source": [
    "Use the fitted `binarySVM` to make predictions for `X_test_ss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9192f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binarySVM.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f94ac9",
   "metadata": {},
   "source": [
    "Model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5abfdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9590643274853801\n",
      "F1 score: 0.9671361502347416\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd052ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.97      0.95        63\n",
      "           1       0.98      0.95      0.97       108\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.95      0.96      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a5558",
   "metadata": {},
   "source": [
    "We obtained a good model performance using our implementation of linear SVM for binary classfication. Now we are going to build a multi-class classifier using the linear SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a7942",
   "metadata": {},
   "source": [
    "## 2. Building a Multi-class Classfier using linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68402fc",
   "metadata": {},
   "source": [
    "We will use the One-vs-Rest approach. If the number of classes is n, then we will have n hyperplanes, each separating one class from the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817105a8",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef5e90",
   "metadata": {},
   "source": [
    "To implement a multi-class linear SVM classfier, we write a Python class which is initiated with 4 parameters (same as the binary linear SVM class):\n",
    "\n",
    "- C: the regularization parameter\n",
    "\n",
    "- alpha: learning rate for the gradient descent algorithm\n",
    "\n",
    "- max_its: number of iterations for updating the weights\n",
    "\n",
    "- random_state: random seed for initializing $w_0$\n",
    "\n",
    "If the `fit` function is called on a dataset with n classes, n binary linear SVMs will be fit. Each binary SVM predicts whether a data point belongs to one class (1) or not (-1). The n set of fitted weights (w) and biases (b) will be recorded for prediction use.\n",
    "\n",
    "If the `predict` function is called, we compute n one dimensional array of predicted values with: $w^T x + b$. For each data point, it will be classified as the class that produces the largest value of $w^T x + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79914e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLinearSVM():\n",
    "    \n",
    "    def __init__(self, C, alpha, max_its, random_state):\n",
    "        \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_its = max_its\n",
    "        self.random_state = random_state\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.binarySVMs = []\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # fit n_classes SVMs and record and fitted weights and biases \n",
    "        self.classes = list(set(y))\n",
    "        for cls in self.classes:\n",
    "            \n",
    "            y_bin = np.array([1 if c==cls else -1 for c in y])\n",
    "            \n",
    "            binarySVM = BinaryLinearSVM(C=self.C, alpha=self.alpha, max_its=self.max_its, random_state=self.random_state)\n",
    "            binarySVM.fit(X, y_bin)\n",
    "            \n",
    "            self.binarySVMs.append(binarySVM)         \n",
    "            self.weights.append(binarySVM.weight)\n",
    "            self.biases.append(binarySVM.bias)\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        n_classes = len(self.classes)     \n",
    "        preds_matrix = np.zeros((n_classes, X.shape[0])) # n_classes x n_observations\n",
    "        \n",
    "        X = X.T\n",
    "        for i in range(n_classes):\n",
    "            \n",
    "            # compute value of wx+b for each class for all the data points, store in preds_matrix\n",
    "            preds = self.weights[i].T @ X + self.biases[i]\n",
    "            preds_matrix[i, :] = preds\n",
    "        \n",
    "        # data point will be classified as the class that has the largest value of wx+b\n",
    "        pred_labels = [self.classes[i] for i in np.argmax(preds_matrix, axis=0)]\n",
    "        \n",
    "        return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0af209",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "\n",
    "We will use the iris dataset from scikit-learn. The iris dataset has three classes for three types of irises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28399127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcebcc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a90265",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd59b5",
   "metadata": {},
   "source": [
    "We keep 70% of the data as training set and 30% of the data as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a66df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (45, 4) (105,) (45,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdca94",
   "metadata": {},
   "source": [
    "We scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eddd81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815983a3",
   "metadata": {},
   "source": [
    "### Multi-class SVM model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00578f4",
   "metadata": {},
   "source": [
    "We create a multi-class linear SVM classifier object from the `MulticlassLinearSVM` class and use it to fit the standardized training data `X_train_ss` and `y_train`. We set the regularization parameter to 1.0, gradient descent learning rate to 0.01, and number of iterations to 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "866b159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclassSVM = MulticlassLinearSVM(C=1, alpha=0.01, max_its=2000, random_state=123)\n",
    "multiclassSVM.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c16072",
   "metadata": {},
   "source": [
    "Use the fitted `multiclassSVM` to make predictions for `X_test_ss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbb068f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multiclassSVM.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4b01e",
   "metadata": {},
   "source": [
    "Model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46cd920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9777777777777777\n",
      "F1 score: 0.9776182336182336\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a01b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b1e1a",
   "metadata": {},
   "source": [
    "As you can see, the performance of our fitted `multiclassSVM` is very good. It has a high precision and recall score for all three classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494c772",
   "metadata": {},
   "source": [
    "## 3. Comparing the Multi-class SVM Classifier with Off-shelf SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635045a",
   "metadata": {},
   "source": [
    "We will compare the performance of our `MulticlassLinearSVM` class with the off-the-shelf linear SVM classifier from [scikit-learn](https://scikit-learn.org/stable/modules/svm.html) on the iris dataset, in terms of training time and error metrics. \n",
    "\n",
    "We import the svm from the `sklearn` package. The `LinearSVC` model also tries to minimizes the squared hinge loss, and it also use the One-vs-Rest approach to handle multi-class classification. The methodology is similar to our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c51f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175939b",
   "metadata": {},
   "source": [
    "### Compare training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c414e",
   "metadata": {},
   "source": [
    "Both will train for 2000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69edac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Off-the-shelf SVM classifier took 0.0 seconds to fit the training data\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "off_shelf_svm = svm.LinearSVC(C=1.0, max_iter=2000)\n",
    "off_shelf_svm.fit(X_train_ss, y_train)\n",
    "t_end = time.time()\n",
    "print(f\"Off-the-shelf SVM classifier took {np.round(t_end - t_start, 2)} seconds to fit the training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a0c4d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class linear SVM classifier took 19.06 seconds to fit the training data\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "multiclassSVM = MulticlassLinearSVM(C=1, alpha=0.1, max_its=2000, random_state=123)\n",
    "multiclassSVM.fit(X_train_ss, y_train)\n",
    "t_end = time.time()\n",
    "print(f\"Multi-class linear SVM classifier took {np.round(t_end - t_start, 2)} seconds to fit the training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31e28e",
   "metadata": {},
   "source": [
    "### Compare performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "625fe723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Off-the-shelf SVM classifier:\n",
      "\n",
      "accuracy score: 0.9555555555555556\n",
      "F1 score: 0.9552910052910052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_1 = off_shelf_svm.predict(X_test_ss)\n",
    "print(\"Off-the-shelf SVM classifier:\\n\")\n",
    "print(f\"accuracy score: {accuracy_score(y_test, y_pred_1)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, y_pred_1, average='weighted')}\")\n",
    "print(classification_report(y_test, y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cf27da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class linear SVM classifier:\n",
      "\n",
      "accuracy score: 0.9111111111111111\n",
      "F1 score: 0.9072631072631072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.90        19\n",
      "           1       1.00      0.69      0.82        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.94      0.90      0.91        45\n",
      "weighted avg       0.93      0.91      0.91        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_2 = multiclassSVM.predict(X_test_ss)\n",
    "print(\"Multi-class linear SVM classifier:\\n\")\n",
    "print(f\"accuracy score: {accuracy_score(y_test, y_pred_2)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, y_pred_2, average='weighted')}\")\n",
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd8fb2",
   "metadata": {},
   "source": [
    "Surprisingly, even though our hand-crafted multi-class linear SVM classifier takes significantly more time to train, it produces a similar model performance to the off-the-shelf SVM (just slightly inferior), given the same regularization value and the same number of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b42c38",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01254b51",
   "metadata": {},
   "source": [
    "It's apparent that the training of our hand-crafted model takes too long compared to the off-the-shelf model. \n",
    "\n",
    "To reduce the training time of our hand-crafted model, one method we could use is early-stopping. We could add a `tolerance` parameter and a `early_stopping_rounds` parameter to our model class. They can work together in the following way:\n",
    "\n",
    "If, for `early_stopping_rounds` iterations, the cost function of our model still hasn't improved (reduced) by `tolerance` amount, we will early-stop the training. \n",
    "\n",
    "These two parameters can be tuned to help our hand-crafted model achieve better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
